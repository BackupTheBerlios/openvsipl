<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Chapter 8. Parallel Fast Convolution</title><link rel="stylesheet" href="cs.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.74.0"><meta name="description" content="This chapter describes how to create and run parallel VSIPL++ programs with Sourcery VSIPL++. You can modify the programs to develop your own parallel applications."><link rel="home" href="index.html" title="Sourcery VSIPL++"><link rel="up" href="pt02.html" title="Part II. Example Application"><link rel="prev" href="sec-io-extdata.html" title="7.4. Performing I/O with External Data Access"><link rel="next" href="ch08s02.html" title="8.2. Improving Parallel Temporal Locality"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Chapter 8. Parallel Fast Convolution</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="sec-io-extdata.html">Prev</a> </td><th width="60%" align="center">Part II. Example Application</th><td width="20%" align="right"> <a accesskey="n" href="ch08s02.html">Next</a></td></tr></table><hr></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="chap-parallel"></a>Chapter 8. Parallel Fast Convolution</h2></div><div><div class="abstract"><p class="title"><b>Abstract</b></p><p>
    This chapter describes how to create and run parallel VSIPL++
    programs with Sourcery VSIPL++.  You can modify the programs to
    develop your own parallel applications.
   </p></div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="chap-parallel.html#sec-parallel-fastconv">8.1. Parallel Fast Convolution</a></span></dt><dt><span class="section"><a href="ch08s02.html">8.2. Improving Parallel Temporal Locality</a></span></dt><dt><span class="section"><a href="sec-parallel-io.html">8.3. Performing I/O</a></span></dt></dl></div><p>
  This chapter explains how to use Sourcery VSIPL++ to perform
  parallel computations.  You will see how to transform the
  fast convolution program from the previous chapter to run
  in parallel.  First you will convert the <code class="function">Fftm</code>
  based version.  Then you will convert the improved cache
  locality version.  Finally, you will learn how to handle
  input and output when working in parallel.
 </p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="sec-parallel-fastconv"></a>8.1. Parallel Fast Convolution</h2></div></div></div><p>
   The first fast convolution program in the previous chapter makes
   use of two implicitly parallel operators: <code class="function">Fftm</code> and
   <code class="function">vmmul</code>.  These operators are implicitly parallel
   in the sense that they process each row of the matrix
   independently.  If you had enough processors, you could put each
   row on a separate processor and then perform the entire
   computation in parallel.
  </p><p>
   In the VSIPL++ API, you have explicit control of the number of
   processors used for a computation.  Since the default is to use
   just a single processor, the program in <a class="xref" href="chap-serial.html#sec-serial-fastconv" title="7.1. Fast Convolution">Section 7.1, &#8220;Fast Convolution&#8221;</a> will not run in parallel, even on a
   multi-processor system.  This section will show you how to use
   <em class="firstterm">maps</em> to take advantage of multiple
   processors.  Using a map tells Sourcery VSIPL++ to distribute a
   single block of data across multiple processors.  Then, Sourcery
   VSIPL++ will automatically move data between processors as
   necessary.
  </p><p>
   The VSIPL++ API uses the Single-Program Multiple-Data (SPMD) model
   for parallelism.  In this model, every processor runs the same
   program, but operates on different sets of data.  For example, in
   the fast convolution example, multiple processors perform FFTs at
   the same time, but each processor handles different rows in the
   matrix.
  </p><p>
   Every map has both compile-time and run-time properties.  At
   compile-time, you specify the <em class="firstterm">distribution</em>
   that will be applied to each dimension.  In this example, you will
   use a <em class="firstterm">block distribution</em> to distribute the
   rows of the matrix.  A block distribution divides a view into
   continguous chunks.  For example, suppose that you have a
   4-processor system.  Since there are 64 rows in the matrix
   <code class="varname">data</code>, there will be 16 rows on each processor.
   The block distribution will place the first 16 rows (rows 0 through
   15) on processor 0, the next 16 rows (rows 16 through 31) on
   processor 1, and so forth.  You do not want to distribute the
   columns of the matrix at all, so you will use a <em class="firstterm">whole
   distribution</em> for the columns.
  </p><p>
   Although the distributions are selected at compile-time, the number
   of processors to use in each dimension is not specified until
   run-time.  By specifying the number of processors at run-time, you
   can adapt your program to the configuration of the machine on which
   your application is running.  The VSIPL++ API provides a
   <code class="function">num_processors</code> function to tell you the total
   number of processors available.  Of course, since each row should
   be kept on a single processor, the number of processors used in the
   column dimension is just one.  So, here is the code required to
   create the map:
  </p><pre class="programlisting">  typedef Map&lt;Block_dist, Whole_dist&gt;               map_type;
  map_type map = map_type(/*rows=*/num_processors(), 
                          /*columns=*/1);</pre><p>
   Next, you have to tell Sourcery VSIPL++ to use this map for the relevant
   views.  Every view has an underlying <em class="firstterm">block</em>.
   The block indicates how the view's data is stored.  Until this
   point, you have been using the default <code class="classname">Dense</code>
   block, which stores data in a continguous array on a single
   processor.  Now, you want to use a continguous array on
   <span class="emphasis"><em>multiple</em></span> processors, so you must explicitly
   distribute the block.  Then, when declaring views, you must
   explicitly indicate that the view should use the distributed block:
  </p><pre class="programlisting">  typedef Dense&lt;2, value_type, row2_type, map_type&gt; block_type;
  typedef Matrix&lt;value_type, block_type&gt;            view_type;
  view_type data(npulse, nrange, map);
  view_type tmp(npulse, nrange, map);</pre><p>
   Performing the vector-matrix multiply requires a complete copy of
   <code class="varname">replica</code> on each processor.  An ordinary map
   divides data among processors, but, here, the goal is to copy the
   same data to multiple processors.  Sourcery VSIPL++ provides a
   special <code class="classname">Replicated_map</code> class to use in this
   situation.  So, you should declare <code class="varname">replica</code> as
   follows:
  </p><pre class="programlisting">  Replicated_map&lt;1&gt; replica_map;
  typedef Dense&lt;1, value_type, row1_type, Replicated_map&lt;1&gt; &gt;
                                                    replica_block_type;
  typedef Vector&lt;value_type, replica_block_type&gt;    replica_view_type;
  replica_view_type replica(nrange, replica_map);</pre><p>
   Because the application already uses implicitly parallel operators,
   no further changes are required.  The entire algorithm (i.e., the
   part of the code that performs FFTs and vector-matrix
   multiplication) remains unchanged.
  </p><p>
   The complete parallel program is:
  </p><pre class="programlisting">/***********************************************************************
  Included Files
***********************************************************************/

#include &lt;vsip/initfin.hpp&gt;
#include &lt;vsip/support.hpp&gt;
#include &lt;vsip/signal.hpp&gt;
#include &lt;vsip/math.hpp&gt;
#include &lt;vsip/map.hpp&gt;

using namespace vsip;



/***********************************************************************
  Main Program
***********************************************************************/

int
main(int argc, char** argv)
{
  // Initialize the library.
  vsipl vpp(argc, argv);

  typedef complex&lt;float&gt; value_type;

  typedef Map&lt;Block_dist, Whole_dist&gt;               map_type;
  typedef Dense&lt;2, value_type, row2_type, map_type&gt; block_type;
  typedef Matrix&lt;value_type, block_type&gt;            view_type;

  typedef Dense&lt;1, value_type, row1_type, Replicated_map&lt;1&gt; &gt;
                                                    replica_block_type;
  typedef Vector&lt;value_type, replica_block_type&gt;    replica_view_type;

  // Parameters.
  length_type npulse = 64;	// number of pulses
  length_type nrange = 256;	// number of range cells

  // Maps.
  map_type          map = map_type(num_processors(), 1);
  Replicated_map&lt;1&gt; replica_map;

  // Views.
  replica_view_type replica(nrange, replica_map);
  view_type         data(npulse, nrange, map);
  view_type         tmp (npulse, nrange, map);

  // A forward Fft for computing the frequency-domain version of
  // the replica.
  typedef Fft&lt;const_Vector, value_type, value_type, fft_fwd, by_reference&gt;
		for_fft_type;
  for_fft_type  for_fft (Domain&lt;1&gt;(nrange), 1.0);

  // A forward Fftm for converting the time-domain data matrix to the
  // frequency domain.
  typedef Fftm&lt;value_type, value_type, row, fft_fwd, by_reference&gt;
	  	for_fftm_type;
  for_fftm_type for_fftm(Domain&lt;2&gt;(npulse, nrange), 1.0);

  // An inverse Fftm for converting the frequency-domain data back to
  // the time-domain.
  typedef Fftm&lt;value_type, value_type, row, fft_inv, by_reference&gt;
	  	inv_fftm_type;
  inv_fftm_type inv_fftm(Domain&lt;2&gt;(npulse, nrange), 1.0/(nrange));

  // Initialize data to zero.
  data    = value_type();
  replica = value_type();

  // Before fast convolution, convert the replica to the the
  // frequency domain
  for_fft(replica);


  // Perform fast convolution:

  // Convert to the frequency domain.
  for_fftm(data, tmp);

  // Perform element-wise multiply for each pulse.
  tmp = vmmul&lt;0&gt;(replica, tmp);

  // Convert back to the time domain.
  inv_fftm(tmp, data);
}
 </pre><p>
   The following graph shows the parallel speedup of the fast
   convolution program from 1 to 32 processors using a 3.0 GHz Pentium
   cluster system.  As you can see, increasing the number of
   processors also increases the performance of the program.
  </p><div class="mediaobject" align="center"><img src="images/par/fastconv-speedup.png" align="middle"></div></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="sec-io-extdata.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="pt02.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="ch08s02.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">7.4. Performing I/O with External Data Access </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> 8.2. Improving Parallel Temporal Locality</td></tr></table></div></body></html>
