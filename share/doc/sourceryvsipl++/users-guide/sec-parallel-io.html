<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>8.3. Performing I/O</title><link rel="stylesheet" href="cs.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.74.0"><link rel="home" href="index.html" title="Sourcery VSIPL++"><link rel="up" href="chap-parallel.html" title="Chapter 8. Parallel Fast Convolution"><link rel="prev" href="ch08s02.html" title="8.2. Improving Parallel Temporal Locality"><link rel="next" href="apa.html" title="Appendix A. Benchmark Options"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">8.3. Performing I/O</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ch08s02.html">Prev</a> </td><th width="60%" align="center">Chapter 8. Parallel Fast Convolution</th><td width="20%" align="right"> <a accesskey="n" href="apa.html">Next</a></td></tr></table><hr></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="sec-parallel-io"></a>8.3. Performing I/O</h2></div></div></div><p>
   The previous sections have ignored the acquisition of actual sensor
   data by setting the input data to zero.  This section shows how to
   extend the I/O techniques introduced in the previous chapter to
   initialize <code class="code">data</code> before performing the fast convolution.
  </p><p>
   Let's assume that all of the input data arrives at a single
   processor via DMA.  This data must be distributed to the other
   processors to perform the fast convolution.  So, the input
   processor is special, and is not involved in the computation
   proper.
  </p><p>
   To describe this situation in Sourcery VSIPL++, you need two maps: one 
   for the input processor (<code class="varname">map_in</code>), and one for
   the compute processors (<code class="varname">map</code>).  These two maps
   will be used to define views that can be used to move the data from
   the input processor to the compute processors.  Let's assume that
   the input processor is processor zero.  Then, create
   <code class="varname">map_in</code> as follows, mapping all data to the
   single input processor:
  </p><pre class="programlisting">  typedef Map&lt;&gt; map_type;
  Vector&lt;processor_type&gt; pvec_in(1);  pvec_in(0)  = 0;
  map_type map_in (pvec_in,  1,  1);</pre><p>
   In contrast, <code class="varname">map</code> distributes rows across all of
   the compute processors:
  </p><pre class="programlisting">  // Distribute computation across all processors:
  map_type map    (num_processors(), 1);</pre><p>
   Because the data will be arriving via DMA, you must explicitly
   manage the memory used by Sourcery VSIPL++.  Because VSIPL++ uses the
   SPMD model, each processor must allocate
   the memory for its local portion the input block, even though all
   processors except the actual input processor will allocate zero
   bytes.  The code required to
   set up the views is:
  </p><pre class="programlisting">  block_type data_in_block(npulse, nrange, 0, map_in);
  view_type data_in(data_in_block);
  view_type data   (npulse, nrange, map);
  size_t size = subblock_domain(data_in).size();
  std::vector&lt;value_type&gt; buffer(size);
  data_in.block()-&gt;rebind(&amp;buffer.front());</pre><p>
   Now, you can perform the actual I/O.  The I/O (including any calls
   to low-level DMA routines) should only be performed on the input
   processor.  The <code class="function">subblock</code> function is used to
   ensure that I/O is only performed on the appropriate processors:
  </p><pre class="programlisting">  if (subblock(data_in) != no_subblock)
   {
     data_in.block().release(false);
     // ... perform IO into data_in ...
     data_in.block().admit(true);
   }</pre><p>
   Once the I/O completes, you can move the data from
   <code class="code">data_in</code> to <code class="code">data</code> for processing.  In the
   VSIPL++ API, ordinary assignment (using the <code class="code">=</code>
   operator) will perform all communication necessary to distribute
   the data.  So, performing the "scatter" operation is
   just:
  </p><pre class="programlisting">  data = data_in;</pre><p>
   The complete program is:
  </p><pre class="programlisting">/***********************************************************************
  Included Files
***********************************************************************/

#include &lt;vsip/initfin.hpp&gt;
#include &lt;vsip/support.hpp&gt;
#include &lt;vsip/signal.hpp&gt;
#include &lt;vsip/math.hpp&gt;
#include &lt;vsip/map.hpp&gt;
#include &lt;vsip/parallel.hpp&gt;

using namespace vsip;



/***********************************************************************
  Main Program
***********************************************************************/

template &lt;typename       ViewT,
	  dimension_type Dim&gt;
ViewT
create_view_wstorage(
  Domain&lt;Dim&gt; const&amp;                          dom,
  typename ViewT::block_type::map_type const&amp; map)
{
  typedef typename ViewT::block_type block_type;
  typedef typename ViewT::value_type value_type;

  block_type* block = new block_type(dom, (value_type*)0, map);
  ViewT view(*block);
  block-&gt;decrement_count();

  if (subblock(view) != no_subblock)
  {
    size_t size = subblock_domain(view).size();
    value_type* buffer = vsip::impl::alloc_align&lt;value_type&gt;(128, size);
    block-&gt;rebind(buffer);
  }

  block-&gt;admit(false);

  return view;
}



template &lt;typename ViewT&gt;
void
cleanup_view_wstorage(ViewT view)
{
  typedef typename ViewT::value_type value_type;
  value_type* ptr;

  view.block().release(false, ptr);
  view.block().rebind((value_type*)0);

  if (ptr) vsip::impl::free_align((void*)ptr);
}



template &lt;typename ViewT&gt;
ViewT
create_view_wstorage(
  length_type                                 rows,
  length_type                                 cols,
  typename ViewT::block_type::map_type const&amp; map)
{
  return create_view_wstorage&lt;ViewT&gt;(Domain&lt;2&gt;(rows, cols), map);
}



template &lt;typename ViewT&gt;
ViewT
create_view_wstorage(
  length_type                                 size,
  typename ViewT::block_type::map_type const&amp; map)
{
  return create_view_wstorage&lt;ViewT&gt;(Domain&lt;1&gt;(size), map);
}




int
main(int argc, char** argv)
{
  // Initialize the library.
  vsipl vpp(argc, argv);

  typedef complex&lt;float&gt; value_type;

  typedef Map&lt;Block_dist, Block_dist&gt;               map_type;
  typedef Dense&lt;2, value_type, row2_type, map_type&gt; block_type;
  typedef Matrix&lt;value_type, block_type&gt;            view_type;

  typedef Dense&lt;1, value_type, row1_type, Replicated_map&lt;1&gt; &gt;
                                                    replica_block_type;
  typedef Vector&lt;value_type, replica_block_type&gt;    replica_view_type;

  typedef Dense&lt;1, value_type, row1_type, Map&lt;&gt; &gt;   replica_io_block_type;
  typedef Vector&lt;value_type, replica_io_block_type&gt; replica_io_view_type;

  // Parameters.
  length_type npulse = 64;	// number of pulses
  length_type nrange = 256;	// number of range cells

  length_type np = num_processors();

  // Processor sets.
  Vector&lt;processor_type&gt; pvec_in(1);  pvec_in(0)  = 0;
  Vector&lt;processor_type&gt; pvec_out(1); pvec_out(0) = np-1;

  // Maps.
  map_type          map_in (pvec_in,  1, 1);
  map_type          map_out(pvec_out, 1, 1);
  map_type          map_row(np, 1);
  Replicated_map&lt;1&gt; replica_map;

  // Views.
  view_type data(npulse, nrange, map_row);
  view_type tmp (npulse, nrange, map_row);
  view_type data_in (create_view_wstorage&lt;view_type&gt;(npulse, nrange, map_in));
  view_type data_out(create_view_wstorage&lt;view_type&gt;(npulse, nrange, map_out));
  replica_view_type    replica(nrange);
  replica_io_view_type replica_in(
    create_view_wstorage&lt;replica_io_view_type&gt;(nrange, map_in));

  // A forward Fft for computing the frequency-domain version of
  // the replica.
  typedef Fft&lt;const_Vector, value_type, value_type, fft_fwd, by_reference&gt;
		for_fft_type;
  for_fft_type  for_fft (Domain&lt;1&gt;(nrange), 1.0);

  // A forward Fftm for converting the time-domain data matrix to the
  // frequency domain.
  typedef Fftm&lt;value_type, value_type, row, fft_fwd, by_reference&gt;
	  	for_fftm_type;
  for_fftm_type for_fftm(Domain&lt;2&gt;(npulse, nrange), 1.0);

  // An inverse Fftm for converting the frequency-domain data back to
  // the time-domain.
  typedef Fftm&lt;value_type, value_type, row, fft_inv, by_reference&gt;
	  	inv_fftm_type;
  inv_fftm_type inv_fftm(Domain&lt;2&gt;(npulse, nrange), 1.0/(nrange));

  // Perform input IO
  if (subblock(data_in) != no_subblock)
  {
    data_in.block().release(false);
    // ... perform IO ...
    data_in.block().admit(true);

    replica_in.block().release(false);
    // ... perform IO ...
    replica_in.block().admit(true);

    data_in    = value_type();
    replica_in = value_type();

    // Before fast convolution, convert the replica into the
    // frequency domain
    for_fft(replica_in.local());
  }

  // Scatter data
  data    = data_in;
  replica = replica_in;

  // Perform fast convolution.
  for_fftm(data, tmp);		// Convert to the frequency domain.
  tmp = vmmul&lt;0&gt;(replica, tmp); // Perform element-wise multiply.
  inv_fftm(tmp, data);		// Convert back to the time domain.

  // Gather data
  data_out = data;

  // Perform output IO
  if (subblock(data_out) != no_subblock)
  {
    data_out.block().release(true);
    // ... perform IO ...
    data_out.block().admit(false);
  }

  // Cleanup
  cleanup_view_wstorage(data_in);
  cleanup_view_wstorage(data_out);
  cleanup_view_wstorage(replica_in);
}
</pre><p>
   The technique demonstrated in this section extends easily to the
   situation in which the sensor data is arriving at multiple
   processors simultaneously.  To distribute the I/O across multiple
   processors, just add them to <code class="code">map_in</code>'s processor set
   <code class="code">pvec_in</code>:
  </p><pre class="programlisting">  Vector&lt;processor_type&gt; pvec_in(num_io_proc);
  pvec_in(0)              = 0;
  ...
  pvec_in(num_io_proc-1)  = ...;</pre></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ch08s02.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="chap-parallel.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="apa.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">8.2. Improving Parallel Temporal Locality </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Appendix A. Benchmark Options</td></tr></table></div></body></html>
